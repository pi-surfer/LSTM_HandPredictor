{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#IMPLEMENTED CODE FOR 5 SIGNS\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np   # linear algebra\nimport matplotlib.pylab as plt\n\nimport pandas as pd   # data processing\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nimport tensorflow as tf \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom keras.layers import BatchNormalization, Dropout\nfrom keras.layers import Dense, Activation\nfrom tensorflow import keras\n\n\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\n\n\nplt.rcParams[\"figure.figsize\"] = (16,10)   #Make the plots bigger by default\nplt.rcParams[\"lines.linewidth\"] = 2        #Setting the default line width\nplt.style.use(\"ggplot\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df0 = pd.read_csv(\"../input/emg-4/0.csv\", header=None )\ndf1 = pd.read_csv(\"../input/emg-4/1.csv\", header=None )\ndf2 = pd.read_csv(\"../input/emg-4/2.csv\", header=None )\ndf3 = pd.read_csv(\"../input/emg-4/3.csv\", header=None )\ndf4 = pd.read_csv(\"../input/ninapro.csv\", header=None )   #added\n\ndf = pd.concat([df0,df1,df2,df3,df4], axis = 0)   #modified adding df4\ndf.head()\nprint(df.shape)   #used for debugging. Results: (#rows0 + â€¦ + #rows4, 65)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# rock = 0,\n# scissors = 1,\n# paper = 2,\n# ok =3,\n# sign 5 = 4","metadata":{}},{"cell_type":"code","source":"x = df.loc[:,0:63]\n        \ny = df[64]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.unique()   # y=[0,1,2,3,4] it saves only values which are different one from each other","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array(x)   \ny= np.array(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = x.reshape(x.shape[0]*x.shape[1], 1)\nx.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nx = sc.fit_transform(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = x.reshape((-1, 8, 8))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"All Data size X and y\")\nprint(x.shape)\nprint(y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.eye(np.max(y) + 1)[y]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42,stratify=y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"All Data size X and y\")\nprint(x_train.shape)\nprint(y_train.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test Data size X and y\")\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 8)))   \n\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 64))\nmodel.add(Dense(units = 128))\n\nmodel.add(Dense(units = 5, activation=\"softmax\"))\nmodel.compile(optimizer = \"adam\" , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n\n\nmodel.summary()\n\ntf.keras.utils.plot_model(model, show_shapes = True, to_file=\"model.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs = 250, batch_size = 32, verbose=2 , callbacks=[callback], validation_split=0.2,)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(0)\n\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\nimport seaborn as sns\n\n# Predict the values from the validation dataset:\nY_pred = model.predict(x_test)\n\n# Convert predictions classes to one hot vectors: \nY_pred_classes = np.argmax(Y_pred,axis = 1)   \n#Returns the indices of the maximum values along axis =1.\n\n# Convert validation observations to one hot vectors:\nY_true = np.argmax(y_test,axis = 1) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_true, Y_pred_classes))","metadata":{},"execution_count":null,"outputs":[]}]}